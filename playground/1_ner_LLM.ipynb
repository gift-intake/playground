{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel, field_validator\n",
    "from typing import List\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    jaccard_score,\n",
    "    hamming_loss,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    ")\n",
    "from ollama import chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experment we will benchmark against many differnt models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity(BaseModel):\n",
    "    entity: str\n",
    "    types: str\n",
    "    \n",
    "    @field_validator('entity')\n",
    "    @classmethod\n",
    "    def clean_entity(cls, v):\n",
    "        # Take only the text before the comma if there is one\n",
    "        if isinstance(v, str) and \",\" in v:\n",
    "            return v.split(\",\")[0].strip()\n",
    "        return v\n",
    "\n",
    "class Result(BaseModel):\n",
    "    text: str\n",
    "    entities: List[Entity]\n",
    "\n",
    "class LLMResult(BaseModel):\n",
    "    interval: str\n",
    "    organization: str\n",
    "    money: str\n",
    "    date: str\n",
    "    phone: str\n",
    "    address: str\n",
    "    person: str\n",
    "    faculty: str\n",
    "    payment_method: str\n",
    "    email: str\n",
    "    gift_type: str\n",
    "    frequency: str\n",
    "    distribution: str\n",
    "\n",
    "\n",
    "labels = [\"Interval\", \"Organization\", \"Money\", \"Date\", \"Phone\", \"Address\", \"Person\", \"Faculty\", \"PaymentMethod\", \"Email\", \"Gift Type\", \"Frequency\", \"Distribution\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_path = Path(\"results.jsonl\")\n",
    "\n",
    "results = []\n",
    "\n",
    "with open(test_set_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        raw_data = json.loads(line)\n",
    "        try:\n",
    "            result = Result(**raw_data)\n",
    "            results.append(result)\n",
    "\n",
    "            filtered_entities = [entity for entity in result.entities if entity.types in labels]\n",
    "            \n",
    "            result.entities = filtered_entities\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing line: {e}\")\n",
    "            print(f\"Problematic data: {raw_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"qwen2.5:3b\",\n",
    "    \"deepseek-r1:1.5b\",\n",
    "    \"gemma2:2b\",\n",
    "    \"llama3.2:latest\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(model: str):\n",
    "    llm_results = []\n",
    "    for result in results:\n",
    "        response = chat(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\":\n",
    "            \"\"\"\n",
    "            You are an AI-powered email parsing tool designed to extract donation-related information with high precision and consistency.\n",
    "\n",
    "            ### **Extraction Guidelines:**\n",
    "            - **Currency Detection:** Identify donation amounts in multiple currencies (e.g., $, €, £, ¥).  \n",
    "            - **Name Recognition:** Extract full names and first names.  \n",
    "            - **Interval Parsing:** Detect donation frequencies, including variations like:\n",
    "            - \"monthly\" / \"month\"\n",
    "            - \"yearly\" / \"annual\" / \"per year\"\n",
    "            - \"one-time\" / \"single\" / \"once\"  \n",
    "            - **Faculty Identification:** Extract faculty or department names (e.g., \"Computer Science\", \"Price Faculty of Engineering\").  \n",
    "            - **Payment Method Recognition:** Identify payment methods (e.g., \"credit card\", \"check\", \"bank transfer\", \"cash\").  \n",
    "            - **Gift Type Recognition:** Extract gift types (e.g., \"one-time gift\", \"recurring gift\", \"pledge\", \"payment for a pledge\", \"payment for a recurring gift\").  \n",
    "            - **Distribution Identification:** Recognize how the donation is allocated (e.g., \"scholarship fund\", \"research fund\", \"general fund\"). \n",
    "            - **Email Extraction:** Identify email addresses.\n",
    "\n",
    "            ### **Parsing Considerations:**\n",
    "            - Extract information **regardless of its position in the email** (subject, body, signature).  \n",
    "            - Perform **case-insensitive matching** for all keywords.  \n",
    "            - Handle **variations in amount formatting** (e.g., \"$50\", \"50 USD\", \"50.00\").  \n",
    "\n",
    "            ### **Error Handling:**\n",
    "            - If multiple conflicting values exist, prioritize in this order:  \n",
    "            1. Most recent mention in the email.  \n",
    "            2. Most explicitly stated value.  \n",
    "            3. Full amounts over partial mentions.  \n",
    "            - If a field **cannot be confidently extracted**, return `null` instead of an empty string or an uncertain response.  \n",
    "\n",
    "            ### **Extraction Precision:**\n",
    "            - Aim for **90%+ accuracy** in extracting donation-related details.  \n",
    "            - **Do not guess**—if a value is unclear, return `null`.  \n",
    "            - Ensure all outputs strictly adhere to the JSON format without additional explanations.  \n",
    "\n",
    "                        ### **Output Rules:**\n",
    "            - If a field cannot be confidently identified, return null.\n",
    "            - Do not use empty strings (\"\") or vague statements like \"not specified\" or \"unknown\".\n",
    "            - If the email contains no donation-related information, return an empty JSON object: {}.\n",
    "                ```json\n",
    "                {}\n",
    "                ```\n",
    "            - Do not return any additional text, explanations, or formatting outside the JSON response.\n",
    "            \n",
    "            ### **Output Format:**\n",
    "            You must always return the extracted data in the **following JSON format**, with **no additional text, comments, or explanations**:  \n",
    "\n",
    "            ```json\n",
    "            {\n",
    "            \"interval\": \"[Extracted Interval or null]\",\n",
    "            \"organization\": \"[Extracted Organization or null]\",\n",
    "            \"money\": \"[Extracted Money or null]\",\n",
    "            \"date\": \"[Extracted Date or null]\",\n",
    "            \"phone\": \"[Extracted Phone or null]\",\n",
    "            \"address\": \"[Extracted Address or null]\",\n",
    "            \"person\": \"[Extracted Person or null]\",\n",
    "            \"faculty\": \"[Extracted Faculty or null]\",\n",
    "            \"payment_method\": \"[Extracted Payment Method or null]\",\n",
    "            \"email\": \"[Extracted Email or null]\",\n",
    "            \"gift_type\": \"[Extracted Gift Type or null]\",\n",
    "            \"frequency\": \"[Extracted Frequency or null]\",\n",
    "            \"distribution\": \"[Extracted Distribution or null]\"\n",
    "            }\n",
    "            \"\"\"},\n",
    "                {\"role\": \"user\", \"content\": result.text}\n",
    "            ],\n",
    "            model=model,\n",
    "            format=LLMResult.model_json_schema(),\n",
    "            options={\n",
    "                \"temperature\": 0.0,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        llm_results.append(LLMResult.model_validate_json(response.message.content))\n",
    "    \n",
    "    return llm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = {}\n",
    "\n",
    "for model in models:\n",
    "    model_results[model] = prompt(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_result_to_entities(llm_result):\n",
    "    entities = []\n",
    "    for label in labels:\n",
    "        value = getattr(llm_result, label.lower(), None)\n",
    "        if (\n",
    "            value\n",
    "            and value not in [\"null\", \"\", \"unknown\", \"not specified\", \"n/a\", \"not\"]\n",
    "            and \"unknown\" not in value.lower()\n",
    "        ):\n",
    "            entities.append(Entity(entity=value, types=label))\n",
    "    return entities\n",
    "\n",
    "\n",
    "model_ner_results = {}\n",
    "for model in models:\n",
    "    model_ner_results[model] = [Result(text=\"\", entities=llm_result_to_entities(llm_result)) for llm_result in model_results[model]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(labels)\n",
    "\n",
    "def sort_entities_by_type(results):\n",
    "    for result in results:\n",
    "        result.entities.sort(key=lambda x: x.types)\n",
    "    return results\n",
    "\n",
    "\n",
    "def multi_hot_encode(entities, labels):\n",
    "    \"\"\"Convert a list of entities to a multi-hot encoded vector.\"\"\"\n",
    "    vector = np.zeros(len(labels), dtype=int)\n",
    "    for entity in entities:\n",
    "        if entity.types in labels:\n",
    "            vector[labels.index(entity.types)] = 1\n",
    "    return vector\n",
    "\n",
    "\n",
    "def hamming_score(y_true, y_pred):\n",
    "    \"\"\"Calculate Hamming Score as 1 - Hamming Loss.\"\"\"\n",
    "    return 1 - hamming_loss(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: qwen2.5:3b\n",
      "  Jaccard Score (Macro): 0.5763\n",
      "  Jaccard Score (Micro): 0.6013\n",
      "  Jaccard Score (Samples): 0.6200\n",
      "  Hamming Score (Macro): 0.7685\n",
      "  F1 Score (Macro): 0.6537\n",
      "  F1 Score (Micro): 0.7510\n",
      "  F1 Score (Samples): 0.7467\n",
      "  Precision (Macro): 0.5794\n",
      "  Precision (Micro): 0.6262\n",
      "  Precision (Samples): 0.6429\n",
      "  Recall (Macro): 0.8143\n",
      "  Recall (Micro): 0.9380\n",
      "  Recall (Samples): 0.9259\n",
      "  Accuracy: 0.0700\n",
      "\n",
      "Model: deepseek-r1:1.5b\n",
      "  Jaccard Score (Macro): 0.3061\n",
      "  Jaccard Score (Micro): 0.3766\n",
      "  Jaccard Score (Samples): 0.3595\n",
      "  Hamming Score (Macro): 0.5562\n",
      "  F1 Score (Macro): 0.4250\n",
      "  F1 Score (Micro): 0.5471\n",
      "  F1 Score (Samples): 0.4942\n",
      "  Precision (Macro): 0.3628\n",
      "  Precision (Micro): 0.4411\n",
      "  Precision (Samples): 0.4363\n",
      "  Recall (Macro): 0.6424\n",
      "  Recall (Micro): 0.7200\n",
      "  Recall (Samples): 0.7330\n",
      "  Accuracy: 0.0000\n",
      "\n",
      "Model: gemma2:2b\n",
      "  Jaccard Score (Macro): 0.5776\n",
      "  Jaccard Score (Micro): 0.6070\n",
      "  Jaccard Score (Samples): 0.6235\n",
      "  Hamming Score (Macro): 0.7812\n",
      "  F1 Score (Macro): 0.6466\n",
      "  F1 Score (Micro): 0.7555\n",
      "  F1 Score (Samples): 0.7505\n",
      "  Precision (Macro): 0.5956\n",
      "  Precision (Micro): 0.6468\n",
      "  Precision (Samples): 0.6620\n",
      "  Recall (Macro): 0.7697\n",
      "  Recall (Micro): 0.9081\n",
      "  Recall (Samples): 0.8967\n",
      "  Accuracy: 0.0800\n",
      "\n",
      "Model: llama3.2:latest\n",
      "  Jaccard Score (Macro): 0.5633\n",
      "  Jaccard Score (Micro): 0.5985\n",
      "  Jaccard Score (Samples): 0.5959\n",
      "  Hamming Score (Macro): 0.7796\n",
      "  F1 Score (Macro): 0.6381\n",
      "  F1 Score (Micro): 0.7488\n",
      "  F1 Score (Samples): 0.7315\n",
      "  Precision (Macro): 0.5770\n",
      "  Precision (Micro): 0.6504\n",
      "  Precision (Samples): 0.6490\n",
      "  Recall (Macro): 0.7544\n",
      "  Recall (Micro): 0.8822\n",
      "  Recall (Samples): 0.8733\n",
      "  Accuracy: 0.0400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_scores = {}\n",
    "\n",
    "for model, ner_results in model_ner_results.items():\n",
    "    # Ensure sorting for consistency\n",
    "    sorted_results = sort_entities_by_type(results)  # Ground truth\n",
    "    sorted_ner_results = sort_entities_by_type(ner_results)  # Model predictions\n",
    "\n",
    "    # Multi-hot encode ground truth and predictions\n",
    "    y_true = np.array([multi_hot_encode(gt.entities, labels) for gt in sorted_results])\n",
    "    y_pred = np.array(\n",
    "        [multi_hot_encode(pred.entities, labels) for pred in sorted_ner_results]\n",
    "    )\n",
    "\n",
    "    # Compute Jaccard and Hamming scores\n",
    "    jaccard_macro = jaccard_score(y_true, y_pred, average=\"macro\")\n",
    "    jaccard_micro = jaccard_score(y_true, y_pred, average=\"micro\")\n",
    "    jaccard_samples = jaccard_score(y_true, y_pred, average=\"samples\")\n",
    "    hamming_macro = hamming_score(y_true, y_pred)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "    f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "    f1_samples = f1_score(y_true, y_pred, average='samples')\n",
    "\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    precision_micro = precision_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    precision_samples = precision_score(y_true, y_pred, average='samples', zero_division=0)\n",
    "\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall_micro = recall_score(y_true, y_pred, average='micro', zero_division=0)\n",
    "    recall_samples = recall_score(y_true, y_pred, average='samples', zero_division=0)\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "    # Store results\n",
    "    model_scores[model] = {\n",
    "        \"Jaccard Score (Macro)\": jaccard_macro,\n",
    "        \"Jaccard Score (Micro)\": jaccard_micro,\n",
    "        \"Jaccard Score (Samples)\": jaccard_samples,\n",
    "        \"Hamming Score (Macro)\": hamming_macro,\n",
    "        \"F1 Score (Macro)\": f1_macro,\n",
    "        \"F1 Score (Micro)\": f1_micro,\n",
    "        \"F1 Score (Samples)\": f1_samples,\n",
    "        \"Precision (Macro)\": precision_macro,\n",
    "        \"Precision (Micro)\": precision_micro,\n",
    "        \"Precision (Samples)\": precision_samples,\n",
    "        \"Recall (Macro)\": recall_macro,\n",
    "        \"Recall (Micro)\": recall_micro,\n",
    "        \"Recall (Samples)\": recall_samples,\n",
    "        \"Accuracy\": accuracy,\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for model, scores in model_scores.items():\n",
    "    print(f\"Model: {model}\")\n",
    "    for metric, score in scores.items():\n",
    "        print(f\"  {metric}: {score:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
